{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (4.45.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 9, 9, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 9, 9, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 41472)             0         \n",
      "=================================================================\n",
      "Total params: 1,572,416\n",
      "Trainable params: 1,570,496\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 64, 64, 3)         3459      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 2,043,011\n",
      "Trainable params: 2,041,475\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [Discriminator loss: 3.200625, acc.: 232.08%] [Generator loss: 0.953959]\n",
      "fid : 262.3345162310587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [Discriminator loss: 1.689788, acc.: 80.26%] [Generator loss: 2.967598]\n",
      "fid : 383.0637691404558\n",
      "2 [Discriminator loss: 2.055300, acc.: 114.02%] [Generator loss: 2.722425]\n",
      "fid : 367.6512069026601\n",
      "3 [Discriminator loss: 2.408659, acc.: 133.04%] [Generator loss: 2.341868]\n",
      "fid : 386.6663649949729\n",
      "4 [Discriminator loss: 1.751449, acc.: 98.60%] [Generator loss: 1.393030]\n",
      "fid : 427.7952506441568\n",
      "5 [Discriminator loss: 1.977555, acc.: 105.55%] [Generator loss: 1.428542]\n",
      "fid : 332.9711690708229\n",
      "6 [Discriminator loss: 1.565635, acc.: 58.01%] [Generator loss: 1.290062]\n",
      "fid : 452.5356532808279\n",
      "7 [Discriminator loss: 1.628902, acc.: 60.97%] [Generator loss: 1.079920]\n",
      "fid : 392.2731175238348\n",
      "8 [Discriminator loss: 1.852274, acc.: 65.84%] [Generator loss: 1.203897]\n",
      "fid : 439.19445545482074\n",
      "9 [Discriminator loss: 1.241583, acc.: 31.26%] [Generator loss: 1.237868]\n",
      "fid : 465.38889862693406\n",
      "10 [Discriminator loss: 1.380788, acc.: 34.89%] [Generator loss: 1.796255]\n",
      "fid : 465.76879760520535\n",
      "11 [Discriminator loss: 1.797428, acc.: 58.05%] [Generator loss: 3.327243]\n",
      "fid : 525.873281441145\n",
      "12 [Discriminator loss: 1.534561, acc.: 43.93%] [Generator loss: 2.375717]\n",
      "fid : 625.1106182042606\n",
      "13 [Discriminator loss: 1.994040, acc.: 86.53%] [Generator loss: 2.455212]\n",
      "fid : 722.1306677110628\n",
      "14 [Discriminator loss: 2.110345, acc.: 79.42%] [Generator loss: 3.175364]\n",
      "fid : 870.5881651833629\n",
      "15 [Discriminator loss: 1.977639, acc.: 124.10%] [Generator loss: 0.315030]\n",
      "fid : 621.7545630545345\n",
      "16 [Discriminator loss: 1.224205, acc.: 18.15%] [Generator loss: 0.492946]\n",
      "fid : 585.9358327692596\n",
      "17 [Discriminator loss: 1.249574, acc.: 9.45%] [Generator loss: 0.784295]\n",
      "fid : 498.2152287466278\n",
      "18 [Discriminator loss: 1.211231, acc.: 12.39%] [Generator loss: 0.481480]\n",
      "fid : 450.33439872260897\n",
      "19 [Discriminator loss: 1.292250, acc.: 3.02%] [Generator loss: 0.709697]\n",
      "fid : 543.2061067644293\n",
      "20 [Discriminator loss: 1.373681, acc.: 3.10%] [Generator loss: 0.887335]\n",
      "fid : 591.0867156866777\n",
      "21 [Discriminator loss: 1.342236, acc.: 7.64%] [Generator loss: 1.007736]\n",
      "fid : 596.4243147273436\n",
      "22 [Discriminator loss: 1.363015, acc.: 4.12%] [Generator loss: 1.118059]\n",
      "fid : 547.5713567217126\n",
      "23 [Discriminator loss: 1.276827, acc.: 2.54%] [Generator loss: 0.836684]\n",
      "fid : 608.3740433479962\n",
      "24 [Discriminator loss: 1.523305, acc.: 5.30%] [Generator loss: 1.367819]\n",
      "fid : 644.5768319487286\n",
      "25 [Discriminator loss: 1.166311, acc.: 6.58%] [Generator loss: 0.833539]\n",
      "fid : 727.8713337904104\n",
      "26 [Discriminator loss: 1.288936, acc.: 2.01%] [Generator loss: 0.785261]\n",
      "fid : 611.922001851951\n",
      "27 [Discriminator loss: 1.340890, acc.: 1.47%] [Generator loss: 0.610802]\n",
      "fid : 706.324302863339\n",
      "28 [Discriminator loss: 1.514601, acc.: 8.46%] [Generator loss: 0.861197]\n",
      "fid : 841.4411317802973\n",
      "29 [Discriminator loss: 1.445437, acc.: 4.15%] [Generator loss: 1.076686]\n",
      "fid : 679.8599098121792\n",
      "30 [Discriminator loss: 1.323315, acc.: 20.48%] [Generator loss: 2.533510]\n",
      "fid : 1024.4602721951305\n",
      "31 [Discriminator loss: 1.120839, acc.: 18.81%] [Generator loss: 1.529481]\n",
      "fid : 721.0519809527027\n",
      "32 [Discriminator loss: 1.187794, acc.: 17.62%] [Generator loss: 0.573918]\n",
      "fid : 778.1730075939539\n",
      "33 [Discriminator loss: 1.447894, acc.: 42.64%] [Generator loss: 0.744205]\n",
      "fid : 847.0255750333636\n",
      "34 [Discriminator loss: 1.315115, acc.: 35.36%] [Generator loss: 2.383379]\n",
      "fid : 713.1862538093901\n",
      "35 [Discriminator loss: 2.204781, acc.: 97.17%] [Generator loss: 1.730638]\n",
      "fid : 844.026740708783\n",
      "36 [Discriminator loss: 2.135052, acc.: 95.68%] [Generator loss: 1.542122]\n",
      "fid : 811.9564185456235\n",
      "37 [Discriminator loss: 2.261835, acc.: 119.63%] [Generator loss: 2.491336]\n",
      "fid : 668.0716541745661\n",
      "38 [Discriminator loss: 1.348852, acc.: 49.90%] [Generator loss: 0.638811]\n",
      "fid : 644.1624762631244\n",
      "39 [Discriminator loss: 1.450886, acc.: 3.51%] [Generator loss: 0.805748]\n",
      "fid : 521.8557738918057\n",
      "40 [Discriminator loss: 1.334798, acc.: 1.13%] [Generator loss: 0.754881]\n",
      "fid : 439.79787060102933\n",
      "41 [Discriminator loss: 1.379049, acc.: 4.57%] [Generator loss: 0.831904]\n",
      "fid : 450.0153471057208\n",
      "42 [Discriminator loss: 1.338049, acc.: 5.45%] [Generator loss: 0.938176]\n",
      "fid : 490.118745770256\n",
      "43 [Discriminator loss: 1.498545, acc.: 3.81%] [Generator loss: 1.450859]\n",
      "fid : 532.9218577907998\n",
      "44 [Discriminator loss: 1.467415, acc.: 3.04%] [Generator loss: 1.318017]\n",
      "fid : 447.49863151096065\n",
      "45 [Discriminator loss: 1.211206, acc.: 2.67%] [Generator loss: 1.078141]\n",
      "fid : 475.9801323490496\n",
      "46 [Discriminator loss: 1.332108, acc.: 14.12%] [Generator loss: 1.672906]\n",
      "fid : 359.2401437970778\n",
      "47 [Discriminator loss: 0.986141, acc.: 6.14%] [Generator loss: 1.747193]\n",
      "fid : 572.7852712549893\n",
      "48 [Discriminator loss: 1.459486, acc.: 33.19%] [Generator loss: 2.145667]\n",
      "fid : 570.0792959103925\n",
      "49 [Discriminator loss: 1.385036, acc.: 29.08%] [Generator loss: 1.198488]\n",
      "fid : 368.9636581272241\n",
      "50 [Discriminator loss: 1.428137, acc.: 18.25%] [Generator loss: 0.819181]\n",
      "fid : 347.8081365317248\n",
      "51 [Discriminator loss: 1.357147, acc.: 6.82%] [Generator loss: 1.009641]\n",
      "fid : 443.63752880647786\n",
      "52 [Discriminator loss: 1.441602, acc.: 4.09%] [Generator loss: 1.135752]\n",
      "fid : 391.77461967921636\n",
      "53 [Discriminator loss: 1.173985, acc.: 25.51%] [Generator loss: 3.430879]\n",
      "fid : 459.8133865096895\n",
      "54 [Discriminator loss: 1.312837, acc.: 4.03%] [Generator loss: 3.392665]\n",
      "fid : 501.4374777112588\n",
      "55 [Discriminator loss: 1.687650, acc.: 59.52%] [Generator loss: 4.954832]\n",
      "fid : 479.2838748750412\n",
      "56 [Discriminator loss: 1.266102, acc.: 29.56%] [Generator loss: 1.599997]\n",
      "fid : 411.1007844666815\n",
      "57 [Discriminator loss: 0.968583, acc.: 4.41%] [Generator loss: 1.397375]\n",
      "fid : 415.6422145431078\n",
      "58 [Discriminator loss: 1.503070, acc.: 10.38%] [Generator loss: 1.861789]\n",
      "fid : 369.4723028113269\n",
      "59 [Discriminator loss: 1.697816, acc.: 37.87%] [Generator loss: 1.512748]\n",
      "fid : 429.68483686357536\n",
      "60 [Discriminator loss: 0.931757, acc.: 4.98%] [Generator loss: 2.308315]\n",
      "fid : 474.100698959258\n",
      "61 [Discriminator loss: 1.570495, acc.: 57.30%] [Generator loss: 0.823175]\n",
      "fid : 474.9067175576567\n",
      "62 [Discriminator loss: 1.229281, acc.: 24.97%] [Generator loss: 4.912359]\n",
      "fid : 474.3843487887443\n",
      "63 [Discriminator loss: 1.498424, acc.: 30.25%] [Generator loss: 1.407896]\n",
      "fid : 521.5094090650921\n",
      "64 [Discriminator loss: 1.380749, acc.: 50.06%] [Generator loss: 1.724469]\n",
      "fid : 457.4946144232688\n",
      "65 [Discriminator loss: 1.436111, acc.: 34.53%] [Generator loss: 1.043684]\n",
      "fid : 521.7754664484389\n",
      "66 [Discriminator loss: 1.282619, acc.: 6.62%] [Generator loss: 1.064320]\n",
      "fid : 359.1546191853972\n",
      "67 [Discriminator loss: 1.307851, acc.: 11.62%] [Generator loss: 1.383337]\n",
      "fid : 354.8938539368247\n",
      "68 [Discriminator loss: 1.225044, acc.: 5.44%] [Generator loss: 1.892214]\n",
      "fid : 312.12905127568615\n",
      "69 [Discriminator loss: 1.592324, acc.: 41.37%] [Generator loss: 5.832598]\n",
      "fid : 306.1699623257142\n",
      "70 [Discriminator loss: 1.718845, acc.: 39.30%] [Generator loss: 2.739468]\n",
      "fid : 387.40742802811747\n",
      "71 [Discriminator loss: 2.251364, acc.: 116.35%] [Generator loss: 6.921022]\n",
      "fid : 477.57477932256927\n",
      "72 [Discriminator loss: 1.288572, acc.: 57.61%] [Generator loss: 5.887821]\n",
      "fid : 491.5084127294077\n",
      "73 [Discriminator loss: 1.931976, acc.: 38.25%] [Generator loss: 1.765509]\n",
      "fid : 440.53546460365004\n",
      "74 [Discriminator loss: 1.413161, acc.: 2.46%] [Generator loss: 1.643692]\n",
      "fid : 358.7766925732709\n",
      "75 [Discriminator loss: 1.420193, acc.: 23.69%] [Generator loss: 1.196464]\n",
      "fid : 287.53537060989333\n",
      "76 [Discriminator loss: 1.115639, acc.: 1.47%] [Generator loss: 0.803182]\n",
      "fid : 227.9696933224857\n",
      "77 [Discriminator loss: 0.964727, acc.: 3.42%] [Generator loss: 0.714441]\n",
      "fid : 235.43019555267045\n",
      "78 [Discriminator loss: 1.285969, acc.: 14.57%] [Generator loss: 0.928878]\n",
      "fid : 213.3413133131295\n",
      "79 [Discriminator loss: 1.377010, acc.: 11.01%] [Generator loss: 1.250412]\n",
      "fid : 194.18408583676498\n",
      "80 [Discriminator loss: 1.143379, acc.: 11.41%] [Generator loss: 1.081968]\n",
      "fid : 251.30803751820562\n",
      "81 [Discriminator loss: 1.034111, acc.: 8.96%] [Generator loss: 0.805614]\n",
      "fid : 240.51534571326312\n",
      "82 [Discriminator loss: 1.255802, acc.: 11.37%] [Generator loss: 1.302679]\n",
      "fid : 174.4474039006377\n",
      "83 [Discriminator loss: 1.203309, acc.: 25.64%] [Generator loss: 1.691159]\n",
      "fid : 269.24054922271694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 [Discriminator loss: 1.258897, acc.: 31.90%] [Generator loss: 2.191648]\n",
      "fid : 326.5775095537981\n",
      "85 [Discriminator loss: 2.683604, acc.: 156.04%] [Generator loss: 8.350197]\n",
      "fid : 387.9334428621856\n",
      "86 [Discriminator loss: 4.129068, acc.: 281.10%] [Generator loss: 5.743300]\n",
      "fid : 472.34488546953617\n",
      "87 [Discriminator loss: 3.220552, acc.: 197.47%] [Generator loss: 10.720756]\n",
      "fid : 714.0966848841448\n",
      "88 [Discriminator loss: 2.081332, acc.: 71.57%] [Generator loss: 7.496483]\n",
      "fid : 747.9995348947806\n",
      "89 [Discriminator loss: 1.181001, acc.: 7.52%] [Generator loss: 0.655144]\n",
      "fid : 735.243190750054\n",
      "90 [Discriminator loss: 1.868484, acc.: 43.57%] [Generator loss: 5.127024]\n",
      "fid : 633.7492209871016\n",
      "91 [Discriminator loss: 1.177984, acc.: 4.05%] [Generator loss: 6.886938]\n",
      "fid : 522.3695809251926\n",
      "92 [Discriminator loss: 1.466292, acc.: 37.60%] [Generator loss: 3.998409]\n",
      "fid : 500.5416404360859\n",
      "93 [Discriminator loss: 2.159341, acc.: 104.90%] [Generator loss: 8.873137]\n",
      "fid : 366.70159770977887\n",
      "94 [Discriminator loss: 1.558796, acc.: 53.24%] [Generator loss: 8.134275]\n",
      "fid : 326.45049159834986\n",
      "95 [Discriminator loss: 1.650007, acc.: 75.04%] [Generator loss: 3.308432]\n",
      "fid : 251.00880483084836\n",
      "96 [Discriminator loss: 1.510854, acc.: 62.25%] [Generator loss: 2.332518]\n",
      "fid : 288.4554051536851\n",
      "97 [Discriminator loss: 1.532228, acc.: 41.99%] [Generator loss: 5.169546]\n",
      "fid : 480.50560954395206\n",
      "98 [Discriminator loss: 2.785619, acc.: 167.91%] [Generator loss: 5.588242]\n",
      "fid : 638.3005446707266\n",
      "99 [Discriminator loss: 2.400319, acc.: 117.36%] [Generator loss: 1.180554]\n",
      "fid : 533.28171276543\n",
      "100 [Discriminator loss: 1.724115, acc.: 26.33%] [Generator loss: 1.447507]\n",
      "fid : 418.9776353502261\n",
      "101 [Discriminator loss: 1.809163, acc.: 53.68%] [Generator loss: 2.764125]\n",
      "fid : 568.4096890197915\n",
      "102 [Discriminator loss: 0.975401, acc.: 8.66%] [Generator loss: 1.742462]\n",
      "fid : 627.7718528614294\n",
      "103 [Discriminator loss: 1.654120, acc.: 22.62%] [Generator loss: 1.688998]\n",
      "fid : 691.9682647763074\n",
      "104 [Discriminator loss: 1.813556, acc.: 97.77%] [Generator loss: 7.713279]\n",
      "fid : 912.204165376802\n",
      "105 [Discriminator loss: 1.638253, acc.: 49.42%] [Generator loss: 3.980346]\n",
      "fid : 963.5651518853751\n",
      "106 [Discriminator loss: 3.012940, acc.: 187.08%] [Generator loss: 10.338394]\n",
      "fid : 863.4562254782574\n",
      "107 [Discriminator loss: 2.122909, acc.: 99.06%] [Generator loss: 7.923352]\n",
      "fid : 723.3059007404091\n",
      "108 [Discriminator loss: 2.084041, acc.: 78.20%] [Generator loss: 1.368454]\n",
      "fid : 486.44126650238206\n",
      "109 [Discriminator loss: 2.082503, acc.: 104.46%] [Generator loss: 3.200140]\n",
      "fid : 516.9382600312265\n",
      "110 [Discriminator loss: 1.352373, acc.: 26.13%] [Generator loss: 3.955158]\n",
      "fid : 366.644230704733\n",
      "111 [Discriminator loss: 1.919500, acc.: 91.94%] [Generator loss: 1.966445]\n",
      "fid : 477.7377655276598\n",
      "112 [Discriminator loss: 1.808236, acc.: 79.68%] [Generator loss: 5.679330]\n",
      "fid : 634.2023423885487\n",
      "113 [Discriminator loss: 2.161968, acc.: 96.88%] [Generator loss: 4.908248]\n",
      "fid : 460.5541967207986\n",
      "114 [Discriminator loss: 1.829107, acc.: 75.66%] [Generator loss: 4.577583]\n",
      "fid : 516.8357844721143\n",
      "115 [Discriminator loss: 1.456508, acc.: 46.24%] [Generator loss: 1.225033]\n",
      "fid : 525.2080803164897\n",
      "116 [Discriminator loss: 2.453813, acc.: 148.24%] [Generator loss: 3.607306]\n",
      "fid : 333.75029075368326\n",
      "117 [Discriminator loss: 1.125783, acc.: 23.63%] [Generator loss: 4.572491]\n",
      "fid : 370.53203811044466\n",
      "118 [Discriminator loss: 2.591025, acc.: 157.94%] [Generator loss: 5.701712]\n",
      "fid : 284.71154299536204\n",
      "119 [Discriminator loss: 1.928506, acc.: 34.48%] [Generator loss: 6.497588]\n",
      "fid : 327.0563271409395\n",
      "120 [Discriminator loss: 1.361432, acc.: 39.75%] [Generator loss: 1.762410]\n",
      "fid : 279.6600678546529\n",
      "121 [Discriminator loss: 1.698057, acc.: 73.44%] [Generator loss: 3.000151]\n",
      "fid : 317.514087518977\n",
      "122 [Discriminator loss: 1.755173, acc.: 81.72%] [Generator loss: 1.296331]\n",
      "fid : 200.98159165441194\n",
      "123 [Discriminator loss: 2.189284, acc.: 127.07%] [Generator loss: 4.252459]\n",
      "fid : 320.4951474338027\n",
      "124 [Discriminator loss: 2.136920, acc.: 96.29%] [Generator loss: 2.804142]\n",
      "fid : 374.21505818740894\n",
      "125 [Discriminator loss: 1.659243, acc.: 73.70%] [Generator loss: 4.531333]\n",
      "fid : 283.7619072020849\n",
      "126 [Discriminator loss: 1.291635, acc.: 32.96%] [Generator loss: 2.486942]\n",
      "fid : 289.1157660886429\n",
      "127 [Discriminator loss: 2.960210, acc.: 198.57%] [Generator loss: 5.982075]\n",
      "fid : 344.46020849261754\n",
      "128 [Discriminator loss: 2.384648, acc.: 139.08%] [Generator loss: 0.724185]\n",
      "fid : 241.26321849313214\n",
      "129 [Discriminator loss: 1.848136, acc.: 58.81%] [Generator loss: 2.139881]\n",
      "fid : 220.29967866307692\n",
      "130 [Discriminator loss: 1.501223, acc.: 54.94%] [Generator loss: 1.345023]\n",
      "fid : 254.57968335753537\n",
      "131 [Discriminator loss: 1.712468, acc.: 56.02%] [Generator loss: 1.569979]\n",
      "fid : 286.6676126729558\n",
      "132 [Discriminator loss: 1.220104, acc.: 23.37%] [Generator loss: 1.913142]\n",
      "fid : 343.7445472236567\n",
      "133 [Discriminator loss: 1.914929, acc.: 50.26%] [Generator loss: 3.434889]\n",
      "fid : 260.1222944807202\n",
      "134 [Discriminator loss: 1.159123, acc.: 23.98%] [Generator loss: 2.113656]\n",
      "fid : 373.14058000737964\n",
      "135 [Discriminator loss: 1.935074, acc.: 92.45%] [Generator loss: 5.186940]\n",
      "fid : 466.6699600895586\n",
      "136 [Discriminator loss: 1.578301, acc.: 54.64%] [Generator loss: 1.980840]\n",
      "fid : 326.9639113125585\n",
      "137 [Discriminator loss: 2.612128, acc.: 172.46%] [Generator loss: 4.343740]\n",
      "fid : 577.8061205251881\n",
      "138 [Discriminator loss: 1.658569, acc.: 57.58%] [Generator loss: 2.489719]\n",
      "fid : 571.9708808262881\n",
      "139 [Discriminator loss: 3.505891, acc.: 257.77%] [Generator loss: 5.757798]\n",
      "fid : 444.27878215712724\n",
      "140 [Discriminator loss: 2.384928, acc.: 131.27%] [Generator loss: 2.457223]\n",
      "fid : 252.63895587132825\n",
      "141 [Discriminator loss: 1.092138, acc.: 29.76%] [Generator loss: 1.836421]\n",
      "fid : 380.45999277608297\n",
      "142 [Discriminator loss: 1.590343, acc.: 64.45%] [Generator loss: 4.816167]\n",
      "fid : 317.15616388167973\n",
      "143 [Discriminator loss: 1.188589, acc.: 51.12%] [Generator loss: 3.067352]\n",
      "fid : 363.0824230387129\n",
      "144 [Discriminator loss: 1.912800, acc.: 98.52%] [Generator loss: 3.712224]\n",
      "fid : 310.362764600334\n",
      "145 [Discriminator loss: 2.335499, acc.: 135.71%] [Generator loss: 1.768474]\n",
      "fid : 495.8457843699861\n",
      "146 [Discriminator loss: 1.576228, acc.: 61.29%] [Generator loss: 1.993085]\n",
      "fid : 459.67746430521754\n",
      "147 [Discriminator loss: 1.197369, acc.: 42.42%] [Generator loss: 1.842006]\n",
      "fid : 613.1409758306336\n",
      "148 [Discriminator loss: 1.375156, acc.: 60.18%] [Generator loss: 2.366002]\n",
      "fid : 590.7347960908783\n",
      "149 [Discriminator loss: 1.755744, acc.: 51.04%] [Generator loss: 4.021466]\n",
      "fid : 506.61942505777876\n",
      "150 [Discriminator loss: 1.882068, acc.: 92.60%] [Generator loss: 4.147890]\n",
      "fid : 401.842375312528\n",
      "151 [Discriminator loss: 2.978799, acc.: 219.26%] [Generator loss: 6.283013]\n",
      "fid : 318.77658933444496\n",
      "152 [Discriminator loss: 2.106330, acc.: 114.49%] [Generator loss: 3.079938]\n",
      "fid : 202.95763836060718\n",
      "153 [Discriminator loss: 2.557033, acc.: 159.92%] [Generator loss: 1.158333]\n",
      "fid : 169.5790845455595\n",
      "154 [Discriminator loss: 1.299700, acc.: 34.58%] [Generator loss: 3.540313]\n",
      "fid : 209.03690614438372\n",
      "155 [Discriminator loss: 1.425748, acc.: 57.41%] [Generator loss: 2.429819]\n",
      "fid : 166.2499105864036\n",
      "156 [Discriminator loss: 0.951156, acc.: 11.92%] [Generator loss: 2.035198]\n",
      "fid : 159.56891396684642\n",
      "157 [Discriminator loss: 1.251721, acc.: 20.42%] [Generator loss: 3.379278]\n",
      "fid : 241.5060995412033\n",
      "158 [Discriminator loss: 1.271198, acc.: 38.38%] [Generator loss: 5.853377]\n",
      "fid : 290.009160331879\n",
      "159 [Discriminator loss: 1.790518, acc.: 87.94%] [Generator loss: 5.055797]\n",
      "fid : 323.97346135188104\n",
      "160 [Discriminator loss: 1.414428, acc.: 50.28%] [Generator loss: 4.603724]\n",
      "fid : 626.196804966602\n",
      "161 [Discriminator loss: 1.613052, acc.: 63.52%] [Generator loss: 2.115676]\n",
      "fid : 434.46803282857366\n",
      "162 [Discriminator loss: 1.569313, acc.: 71.43%] [Generator loss: 1.722176]\n",
      "fid : 621.079267826029\n",
      "163 [Discriminator loss: 1.091604, acc.: 12.77%] [Generator loss: 1.412730]\n",
      "fid : 707.7428556478378\n",
      "164 [Discriminator loss: 1.196852, acc.: 28.99%] [Generator loss: 0.791771]\n",
      "fid : 524.2358195117752\n",
      "165 [Discriminator loss: 1.542954, acc.: 43.48%] [Generator loss: 2.859280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid : 494.31152209986607\n",
      "166 [Discriminator loss: 1.213094, acc.: 10.14%] [Generator loss: 3.695850]\n",
      "fid : 591.5784891097852\n",
      "167 [Discriminator loss: 0.563998, acc.: 21.01%] [Generator loss: 1.527831]\n",
      "fid : 505.8058141173857\n",
      "168 [Discriminator loss: 1.555374, acc.: 36.53%] [Generator loss: 2.322318]\n",
      "fid : 652.4165905412468\n",
      "169 [Discriminator loss: 1.200702, acc.: 23.15%] [Generator loss: 1.886108]\n",
      "fid : 702.201870833301\n",
      "170 [Discriminator loss: 1.911714, acc.: 90.73%] [Generator loss: 6.179153]\n",
      "fid : 708.2157381878974\n",
      "171 [Discriminator loss: 1.801986, acc.: 68.25%] [Generator loss: 4.472609]\n",
      "fid : 391.0579872813643\n",
      "172 [Discriminator loss: 1.735752, acc.: 85.48%] [Generator loss: 4.812249]\n",
      "fid : 364.9193047605661\n",
      "173 [Discriminator loss: 1.256979, acc.: 32.40%] [Generator loss: 4.966388]\n",
      "fid : 240.07455192386612\n",
      "174 [Discriminator loss: 1.968163, acc.: 101.49%] [Generator loss: 1.852437]\n",
      "fid : 224.94411423206316\n",
      "175 [Discriminator loss: 1.322087, acc.: 44.40%] [Generator loss: 3.585199]\n",
      "fid : 194.30242670557453\n",
      "176 [Discriminator loss: 1.092214, acc.: 13.11%] [Generator loss: 4.752351]\n",
      "fid : 239.46897187102311\n",
      "177 [Discriminator loss: 1.202613, acc.: 33.15%] [Generator loss: 5.523567]\n",
      "fid : 203.58418534101259\n",
      "178 [Discriminator loss: 1.782531, acc.: 77.55%] [Generator loss: 4.828312]\n",
      "fid : 212.09795749995368\n",
      "179 [Discriminator loss: 1.125690, acc.: 28.89%] [Generator loss: 4.914144]\n",
      "fid : 215.3128501016106\n",
      "180 [Discriminator loss: 1.667659, acc.: 67.33%] [Generator loss: 4.715758]\n",
      "fid : 265.27482469654035\n",
      "181 [Discriminator loss: 1.572245, acc.: 59.76%] [Generator loss: 6.052182]\n",
      "fid : 206.62294733510294\n",
      "182 [Discriminator loss: 2.017787, acc.: 109.40%] [Generator loss: 5.535663]\n",
      "fid : 430.12523419897116\n",
      "183 [Discriminator loss: 1.764196, acc.: 92.00%] [Generator loss: 6.525248]\n",
      "fid : 280.5961264836561\n",
      "184 [Discriminator loss: 2.086818, acc.: 109.78%] [Generator loss: 7.624990]\n",
      "fid : 287.11864794520875\n",
      "185 [Discriminator loss: 2.733167, acc.: 168.27%] [Generator loss: 3.117170]\n",
      "fid : 554.8928574271179\n",
      "186 [Discriminator loss: 1.287452, acc.: 49.03%] [Generator loss: 3.226876]\n",
      "fid : 353.3117656279279\n",
      "187 [Discriminator loss: 1.512000, acc.: 49.83%] [Generator loss: 4.659094]\n",
      "fid : 440.3108850832892\n",
      "188 [Discriminator loss: 1.044523, acc.: 35.77%] [Generator loss: 2.261778]\n",
      "fid : 321.47658180911736\n",
      "189 [Discriminator loss: 1.279220, acc.: 53.25%] [Generator loss: 1.580264]\n",
      "fid : 417.44250772057853\n",
      "190 [Discriminator loss: 1.414079, acc.: 54.94%] [Generator loss: 2.623876]\n",
      "fid : 345.99384115227576\n",
      "191 [Discriminator loss: 2.037662, acc.: 109.65%] [Generator loss: 1.010289]\n",
      "fid : 360.70266052624936\n",
      "192 [Discriminator loss: 1.048762, acc.: 27.42%] [Generator loss: 1.098167]\n",
      "fid : 379.61914507066456\n",
      "193 [Discriminator loss: 0.810481, acc.: 14.09%] [Generator loss: 0.879437]\n",
      "fid : 266.6499565562035\n",
      "194 [Discriminator loss: 1.024729, acc.: 4.94%] [Generator loss: 1.293769]\n",
      "fid : 203.02824484451952\n",
      "195 [Discriminator loss: 1.035339, acc.: 35.09%] [Generator loss: 1.126016]\n",
      "fid : 279.0397750828529\n",
      "196 [Discriminator loss: 1.066686, acc.: 16.84%] [Generator loss: 1.563955]\n",
      "fid : 313.65904457243954\n",
      "197 [Discriminator loss: 1.059969, acc.: 15.41%] [Generator loss: 1.445211]\n",
      "fid : 230.29804339195022\n",
      "198 [Discriminator loss: 1.040088, acc.: 19.98%] [Generator loss: 1.172425]\n",
      "fid : 185.3057736628083\n",
      "199 [Discriminator loss: 2.122984, acc.: 129.56%] [Generator loss: 2.736247]\n",
      "fid : 220.23555257247162\n",
      "200 [Discriminator loss: 1.252528, acc.: 50.68%] [Generator loss: 1.249377]\n",
      "fid : 157.0074063014015\n",
      "201 [Discriminator loss: 0.921676, acc.: 15.83%] [Generator loss: 1.472749]\n",
      "fid : 226.8882861140673\n",
      "202 [Discriminator loss: 0.848047, acc.: 16.55%] [Generator loss: 1.016762]\n",
      "fid : 195.27080827417413\n",
      "203 [Discriminator loss: 1.184003, acc.: 51.30%] [Generator loss: 0.996929]\n",
      "fid : 143.71775149104508\n",
      "204 [Discriminator loss: 1.460061, acc.: 61.12%] [Generator loss: 2.325994]\n",
      "fid : 187.0339658881922\n",
      "205 [Discriminator loss: 1.014229, acc.: 41.50%] [Generator loss: 1.103133]\n",
      "fid : 180.68733550053773\n",
      "206 [Discriminator loss: 2.263869, acc.: 143.72%] [Generator loss: 3.206393]\n",
      "fid : 222.92280905633302\n",
      "207 [Discriminator loss: 1.254124, acc.: 54.41%] [Generator loss: 2.894705]\n",
      "fid : 185.73070939308906\n",
      "208 [Discriminator loss: 2.162476, acc.: 119.89%] [Generator loss: 3.068099]\n",
      "fid : 171.59168781322154\n",
      "209 [Discriminator loss: 1.583957, acc.: 77.84%] [Generator loss: 3.035954]\n",
      "fid : 169.22405298825714\n",
      "210 [Discriminator loss: 2.614011, acc.: 174.26%] [Generator loss: 2.095015]\n",
      "fid : 113.15737402345542\n",
      "211 [Discriminator loss: 2.186435, acc.: 138.50%] [Generator loss: 3.789324]\n",
      "fid : 172.12623734056908\n",
      "212 [Discriminator loss: 1.774823, acc.: 84.95%] [Generator loss: 2.429503]\n",
      "fid : 145.56041115770995\n",
      "213 [Discriminator loss: 1.850427, acc.: 95.77%] [Generator loss: 1.950071]\n",
      "fid : 137.97624068686883\n",
      "214 [Discriminator loss: 3.027876, acc.: 223.58%] [Generator loss: 3.101810]\n",
      "fid : 169.9608122187224\n",
      "215 [Discriminator loss: 2.241920, acc.: 117.52%] [Generator loss: 3.746614]\n",
      "fid : 122.6916271384793\n",
      "216 [Discriminator loss: 2.196965, acc.: 126.66%] [Generator loss: 4.195381]\n",
      "fid : 128.44426895709196\n",
      "217 [Discriminator loss: 1.411016, acc.: 63.96%] [Generator loss: 1.978202]\n",
      "fid : 191.56713973766236\n",
      "218 [Discriminator loss: 1.740311, acc.: 74.74%] [Generator loss: 3.665078]\n",
      "fid : 261.3137808499331\n",
      "219 [Discriminator loss: 1.672758, acc.: 68.37%] [Generator loss: 1.599220]\n",
      "fid : 251.68162620947336\n",
      "220 [Discriminator loss: 3.076766, acc.: 194.11%] [Generator loss: 4.057317]\n",
      "fid : 309.196347129347\n",
      "221 [Discriminator loss: 2.183489, acc.: 126.99%] [Generator loss: 2.481669]\n",
      "fid : 264.5091309584028\n",
      "222 [Discriminator loss: 1.535514, acc.: 51.55%] [Generator loss: 3.827021]\n",
      "fid : 356.40725636309804\n",
      "223 [Discriminator loss: 1.283041, acc.: 18.24%] [Generator loss: 3.487255]\n",
      "fid : 382.0953497613734\n",
      "224 [Discriminator loss: 1.231353, acc.: 48.56%] [Generator loss: 2.565497]\n",
      "fid : 359.09432542746714\n",
      "225 [Discriminator loss: 1.348269, acc.: 9.04%] [Generator loss: 2.636364]\n",
      "fid : 538.2259218818359\n",
      "226 [Discriminator loss: 1.211047, acc.: 30.19%] [Generator loss: 3.366324]\n",
      "fid : 631.3628859800534\n",
      "227 [Discriminator loss: 0.811635, acc.: 13.20%] [Generator loss: 3.502948]\n",
      "fid : 587.2372802434028\n",
      "228 [Discriminator loss: 1.391667, acc.: 45.09%] [Generator loss: 1.135479]\n",
      "fid : 458.59487717548046\n",
      "229 [Discriminator loss: 1.643958, acc.: 44.48%] [Generator loss: 3.448265]\n",
      "fid : 494.28422321125686\n",
      "230 [Discriminator loss: 1.106367, acc.: 42.39%] [Generator loss: 1.741724]\n",
      "fid : 489.6178080892124\n",
      "231 [Discriminator loss: 1.422921, acc.: 55.97%] [Generator loss: 0.953198]\n",
      "fid : 426.02659981386125\n",
      "232 [Discriminator loss: 2.240332, acc.: 110.90%] [Generator loss: 2.564107]\n",
      "fid : 335.65164469885195\n",
      "233 [Discriminator loss: 1.251079, acc.: 45.46%] [Generator loss: 2.459669]\n",
      "fid : 315.66843503743877\n",
      "234 [Discriminator loss: 1.564397, acc.: 77.07%] [Generator loss: 2.473691]\n",
      "fid : 311.5934433204674\n",
      "235 [Discriminator loss: 2.532009, acc.: 148.90%] [Generator loss: 2.242095]\n",
      "fid : 191.3243565346338\n",
      "236 [Discriminator loss: 1.591054, acc.: 65.82%] [Generator loss: 4.368927]\n",
      "fid : 197.98303557619485\n",
      "237 [Discriminator loss: 3.084817, acc.: 210.53%] [Generator loss: 3.978856]\n",
      "fid : 175.01239110346592\n",
      "238 [Discriminator loss: 1.509184, acc.: 75.84%] [Generator loss: 2.986647]\n",
      "fid : 175.54232067108535\n",
      "239 [Discriminator loss: 1.088918, acc.: 23.01%] [Generator loss: 2.227432]\n",
      "fid : 307.16353461447596\n",
      "240 [Discriminator loss: 1.622408, acc.: 64.74%] [Generator loss: 2.959478]\n",
      "fid : 148.46883159501584\n",
      "241 [Discriminator loss: 1.516024, acc.: 66.72%] [Generator loss: 3.784581]\n",
      "fid : 189.13125235461425\n",
      "242 [Discriminator loss: 0.805060, acc.: 16.10%] [Generator loss: 2.746613]\n",
      "fid : 166.625066913014\n",
      "243 [Discriminator loss: 1.422385, acc.: 54.59%] [Generator loss: 1.410535]\n",
      "fid : 200.64733328989283\n",
      "244 [Discriminator loss: 1.390081, acc.: 46.47%] [Generator loss: 4.878906]\n",
      "fid : 302.46693342819583\n",
      "245 [Discriminator loss: 1.218788, acc.: 29.28%] [Generator loss: 2.189632]\n",
      "fid : 426.9710602548186\n",
      "246 [Discriminator loss: 1.049244, acc.: 33.70%] [Generator loss: 1.027972]\n",
      "fid : 539.8286919161072\n",
      "247 [Discriminator loss: 1.091146, acc.: 5.15%] [Generator loss: 0.618574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid : 369.6046637052134\n",
      "248 [Discriminator loss: 1.058184, acc.: 8.39%] [Generator loss: 1.078693]\n",
      "fid : 369.5827822317391\n",
      "249 [Discriminator loss: 0.972824, acc.: 9.68%] [Generator loss: 1.341938]\n",
      "fid : 401.4684492586684\n",
      "250 [Discriminator loss: 1.303835, acc.: 15.63%] [Generator loss: 1.647582]\n",
      "fid : 446.0895637512801\n",
      "251 [Discriminator loss: 0.912510, acc.: 12.12%] [Generator loss: 0.924347]\n",
      "fid : 512.95325598296\n",
      "252 [Discriminator loss: 1.845050, acc.: 86.79%] [Generator loss: 6.218782]\n",
      "fid : 343.13205949139785\n",
      "253 [Discriminator loss: 1.816850, acc.: 83.88%] [Generator loss: 3.589729]\n",
      "fid : 319.6653185449536\n",
      "254 [Discriminator loss: 2.697791, acc.: 160.88%] [Generator loss: 4.265374]\n",
      "fid : 264.6093432861599\n",
      "255 [Discriminator loss: 1.253551, acc.: 57.29%] [Generator loss: 2.134433]\n",
      "fid : 341.05415279934766\n",
      "256 [Discriminator loss: 1.816818, acc.: 91.64%] [Generator loss: 2.201337]\n",
      "fid : 491.8018102425938\n",
      "257 [Discriminator loss: 2.038233, acc.: 92.33%] [Generator loss: 2.353734]\n",
      "fid : 217.05373866666335\n",
      "258 [Discriminator loss: 2.897889, acc.: 197.79%] [Generator loss: 2.295770]\n",
      "fid : 124.56090654720506\n",
      "259 [Discriminator loss: 0.950107, acc.: 16.98%] [Generator loss: 2.390020]\n",
      "fid : 194.58573154688406\n",
      "260 [Discriminator loss: 1.022965, acc.: 32.81%] [Generator loss: 0.976029]\n",
      "fid : 117.64344882299098\n",
      "261 [Discriminator loss: 1.188592, acc.: 40.46%] [Generator loss: 1.715121]\n",
      "fid : 122.4525898159406\n",
      "262 [Discriminator loss: 1.039352, acc.: 17.87%] [Generator loss: 1.515107]\n",
      "fid : 120.23342485834904\n",
      "263 [Discriminator loss: 1.206219, acc.: 42.05%] [Generator loss: 0.937442]\n",
      "fid : 137.05022270741352\n",
      "264 [Discriminator loss: 1.019342, acc.: 28.66%] [Generator loss: 1.129496]\n",
      "fid : 138.0998682989112\n",
      "265 [Discriminator loss: 1.041051, acc.: 24.74%] [Generator loss: 1.396528]\n",
      "fid : 104.79049411988603\n",
      "266 [Discriminator loss: 1.307363, acc.: 49.68%] [Generator loss: 2.728189]\n",
      "fid : 119.9612734313202\n",
      "267 [Discriminator loss: 1.569203, acc.: 76.78%] [Generator loss: 3.636679]\n",
      "fid : 223.79026729809095\n",
      "268 [Discriminator loss: 2.557653, acc.: 147.64%] [Generator loss: 5.464170]\n",
      "fid : 207.2726134117714\n",
      "269 [Discriminator loss: 1.271425, acc.: 49.64%] [Generator loss: 3.829808]\n",
      "fid : 191.03600619561553\n",
      "270 [Discriminator loss: 1.944912, acc.: 110.86%] [Generator loss: 4.311269]\n",
      "fid : 180.9932560711855\n",
      "271 [Discriminator loss: 1.149549, acc.: 38.56%] [Generator loss: 2.168546]\n",
      "fid : 244.06625348600238\n",
      "272 [Discriminator loss: 2.238040, acc.: 126.62%] [Generator loss: 3.074459]\n",
      "fid : 177.6814615065594\n",
      "273 [Discriminator loss: 1.901066, acc.: 104.26%] [Generator loss: 3.240056]\n",
      "fid : 186.71367281363587\n",
      "274 [Discriminator loss: 1.452240, acc.: 71.35%] [Generator loss: 2.418442]\n",
      "fid : 210.01099414024762\n",
      "275 [Discriminator loss: 1.195172, acc.: 55.34%] [Generator loss: 2.025763]\n",
      "fid : 147.99932735885855\n",
      "276 [Discriminator loss: 1.453748, acc.: 67.54%] [Generator loss: 1.362195]\n",
      "fid : 230.41256621606297\n",
      "277 [Discriminator loss: 1.151027, acc.: 46.94%] [Generator loss: 2.235651]\n",
      "fid : 247.0613133873752\n",
      "278 [Discriminator loss: 1.393162, acc.: 66.13%] [Generator loss: 2.048230]\n",
      "fid : 228.96612910317944\n",
      "279 [Discriminator loss: 1.694670, acc.: 85.27%] [Generator loss: 1.647957]\n",
      "fid : 181.29691836266835\n",
      "280 [Discriminator loss: 1.004590, acc.: 25.27%] [Generator loss: 1.886047]\n",
      "fid : 154.3409807899554\n",
      "281 [Discriminator loss: 1.577977, acc.: 81.19%] [Generator loss: 1.005368]\n",
      "fid : 199.45220609236648\n",
      "282 [Discriminator loss: 1.148740, acc.: 43.78%] [Generator loss: 1.098465]\n",
      "fid : 152.29921586781182\n",
      "283 [Discriminator loss: 1.918987, acc.: 107.28%] [Generator loss: 1.863521]\n",
      "fid : 184.91539954186317\n",
      "284 [Discriminator loss: 1.627664, acc.: 82.31%] [Generator loss: 2.052661]\n",
      "fid : 184.9835763430453\n",
      "285 [Discriminator loss: 1.721988, acc.: 94.41%] [Generator loss: 1.654670]\n",
      "fid : 243.1231504658176\n",
      "286 [Discriminator loss: 1.658675, acc.: 88.81%] [Generator loss: 2.664580]\n",
      "fid : 207.25713481419186\n",
      "287 [Discriminator loss: 1.321016, acc.: 60.84%] [Generator loss: 2.492869]\n",
      "fid : 218.520315041963\n",
      "288 [Discriminator loss: 1.638066, acc.: 85.31%] [Generator loss: 1.823344]\n",
      "fid : 239.24326722164517\n",
      "289 [Discriminator loss: 1.502850, acc.: 72.57%] [Generator loss: 1.752742]\n",
      "fid : 204.24904606984046\n",
      "290 [Discriminator loss: 1.451961, acc.: 62.18%] [Generator loss: 1.015698]\n",
      "fid : 125.77328094047293\n",
      "291 [Discriminator loss: 1.370086, acc.: 56.66%] [Generator loss: 1.401521]\n",
      "fid : 120.96268834798349\n",
      "292 [Discriminator loss: 1.062821, acc.: 20.33%] [Generator loss: 1.195920]\n",
      "fid : 96.67195225495934\n",
      "293 [Discriminator loss: 1.115758, acc.: 34.96%] [Generator loss: 1.445282]\n",
      "fid : 112.34390106561793\n",
      "294 [Discriminator loss: 1.055603, acc.: 23.69%] [Generator loss: 1.716925]\n",
      "fid : 173.24937685720042\n",
      "295 [Discriminator loss: 0.982845, acc.: 26.16%] [Generator loss: 1.159310]\n",
      "fid : 156.62578842032264\n",
      "296 [Discriminator loss: 0.964245, acc.: 21.01%] [Generator loss: 0.966181]\n",
      "fid : 202.1400381833913\n",
      "297 [Discriminator loss: 1.719428, acc.: 82.83%] [Generator loss: 3.406063]\n",
      "fid : 236.50089583136412\n",
      "298 [Discriminator loss: 1.393663, acc.: 67.85%] [Generator loss: 1.299465]\n",
      "fid : 172.0433454013656\n",
      "299 [Discriminator loss: 1.740481, acc.: 80.81%] [Generator loss: 2.975787]\n",
      "fid : 222.579803640347\n",
      "300 [Discriminator loss: 1.262441, acc.: 29.51%] [Generator loss: 2.527966]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-68c420c00bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mfacegenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mfacegenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_images_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0;31m#facegenerator.generate_single_image(\"saved_models/facegenerator.h5\",\"test.png\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-68c420c00bdf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, datafolder, epochs, batch_size, save_images_interval)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mact1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mact2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fid : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-68c420c00bdf>\u001b[0m in \u001b[0;36mget_fid\u001b[0;34m(act1, act2)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mssdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# calculate sqrt of product between cov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mcovmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;31m# check and correct imaginary numbers from sqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscomplexobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/scipy/linalg/_matfuncs_sqrtm.py\u001b[0m in \u001b[0;36msqrtm\u001b[0;34m(A, disp, blocksize)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mkeep_it_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misrealobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeep_it_real\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrsf2csf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/scipy/linalg/decomp_schur.py\u001b[0m in \u001b[0;36mschur\u001b[0;34m(a, output, lwork, overwrite_a, sort, check_finite)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     result = gees(sfunction, a1, lwork=lwork, overwrite_a=overwrite_a,\n\u001b[0;32m--> 162\u001b[0;31m                   sort_t=sort_t)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!unzip ./train\n",
    "!pip install tqdm\n",
    "#Import everything that is needed from Keras library.\n",
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import glob\n",
    "#matplotlib will help with displaying the results\n",
    "import matplotlib.pyplot as plt\n",
    "#numpy for some mathematical operations\n",
    "import numpy as np\n",
    "#PIL for opening,resizing and saving images\n",
    "from PIL import Image\n",
    "#tqdm for a progress bar when loading the dataset\n",
    "from tqdm import tqdm\n",
    "import scipy as sp\n",
    "#os library is needed for extracting filenames from the dataset folder.\n",
    "import os\n",
    "\n",
    "classnum=15\n",
    "def scale_images(images, new_shape):\n",
    "    images_list = list()\n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = np.resize(image, new_shape)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    return np.asarray(images_list)\n",
    "def get_fid(act1, act2):\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sp.linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "\n",
    "class FaceGenerator:\n",
    "    #RGB-images: 3-channels, grayscale: 1-channel, RGBA-images: 4-channels\n",
    "    def __init__(self,image_width,image_height,channels):\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.image_shape = (self.image_width,self.image_height,self.channels)\n",
    "\n",
    "        #Amount of randomly generated numbers for the first layer of the generator.\n",
    "        self.random_noise_dimension = 100\n",
    "\n",
    "        #Just 10 times higher learning rate would result in generator loss being stuck at 0.\n",
    "        optimizer = Adam(0.0002,0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile( loss=['binary_crossentropy','categorical_crossentropy'],loss_weights=[1., 0.2],\n",
    "        optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        #A placeholder for the generator input.\n",
    "        random_input = Input(shape=(self.random_noise_dimension,))\n",
    "\n",
    "        #Generator generates images from random noise.\n",
    "        generated_image = self.generator(random_input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        #Discriminator attempts to determine if image is real or generated\n",
    "        fake, aux  = self.discriminator(generated_image)\n",
    "\n",
    "        #Combined model = generator and discriminator combined.\n",
    "        #1. Takes random noise as an input.\n",
    "        #2. Generates an image.\n",
    "        #3. Attempts to determine if image is real or generated.\n",
    "        self.combined = Model(random_input,[fake, aux ])\n",
    "        self.combined.compile( loss=['binary_crossentropy','categorical_crossentropy'],loss_weights=[1., 0.2],optimizer=optimizer)\n",
    "\n",
    "    def get_training_data(self,datafolder):\n",
    "        print(\"Loading training data...\")\n",
    "\n",
    "        training_data = []\n",
    "        #Finds all files in datafolder\n",
    "\n",
    "        image_list = []\n",
    "        for filename in glob.glob('./all-dogs/*.jpg'): #assuming gif\n",
    "              \n",
    "              im=Image.open(filename)\n",
    "              image = Image.open(filename).convert('RGB')\n",
    "              #Resizes to a desired size.\n",
    "              image = image.resize((self.image_width,self.image_height),Image.ANTIALIAS)\n",
    "              #Creates an array of pixel values from the image.\n",
    "              pixel_array = np.asarray(image)\n",
    "\n",
    "              training_data.append(pixel_array)\n",
    "       \n",
    "        \n",
    "        \n",
    "        #training_data is converted to a numpy array\n",
    "        training_data = np.reshape(training_data,(-1,self.image_width,self.image_height,self.channels))\n",
    "        return training_data\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "        #Generator attempts to fool discriminator by generating new images.\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256*4*4,activation=\"relu\",input_dim=self.random_noise_dimension))\n",
    "        model.add(Reshape((4,4,256)))\n",
    "\n",
    "        #Four layers of upsampling, convolution, batch normalization and activation.\n",
    "        # 1. Upsampling: Input data is repeated. Default is (2,2). In that case a 4x4x256 array becomes an 8x8x256 array.\n",
    "        # 2. Convolution: If you are not familiar, you should watch this video: https://www.youtube.com/watch?v=FTr3n7uBIuE\n",
    "        # 3. Normalization normalizes outputs from convolution.\n",
    "        # 4. Relu activation:  f(x) = max(0,x). If x < 0, then f(x) = 0.\n",
    "\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "        # Last convolutional layer outputs as many featuremaps as channels in the final image.\n",
    "        model.add(Conv2D(self.channels,kernel_size=3,padding=\"same\"))\n",
    "        # tanh maps everything to a range between -1 and 1.\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        # show the summary of the model architecture\n",
    "        model.summary()\n",
    "\n",
    "        # Placeholder for the random noise input\n",
    "        input = Input(shape=(self.random_noise_dimension,))\n",
    "        #Model output\n",
    "        generated_image = model(input)\n",
    "\n",
    "        #Change the model type from Sequential to Model (functional API) More at: https://keras.io/models/model/.\n",
    "        return Model(input,generated_image)\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        #Discriminator attempts to classify real and generated images\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.image_shape, padding=\"same\"))\n",
    "        #Leaky relu is similar to usual relu. If x < 0 then f(x) = x * alpha, otherwise f(x) = x.\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        #Dropout blocks some connections randomly. This help the model to generalize better.\n",
    "        #0.25 means that every connection has a 25% chance of being blocked.\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        #Zero padding adds additional rows and columns to the image. Those rows and columns are made of zeros.\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dropout(0.25))\n",
    "        #Flatten layer flattens the output of the previous layer to a single dimension.\n",
    "        model.add(Flatten())\n",
    "        #Outputs a value between 0 and 1 that predicts whether image is real or generated. 0 = generated, 1 = real.\n",
    "        \n",
    "        input = Input(shape=self.image_shape)\n",
    "\n",
    "\n",
    "        features = model(input)\n",
    "\n",
    "        # first output (name=generation) is whether or not the discriminator\n",
    "        # thinks the image that is being shown is fake, and the second output\n",
    "        # (name=auxiliary) is the class that the discriminator thinks the image\n",
    "        # belongs to.\n",
    "        fake = Dense(1, activation='sigmoid', name='generation')(features)\n",
    "        aux = Dense(classnum+1, activation='softmax', name='auxiliary')(features)\n",
    "        discriminator=Model(input, [fake, aux])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        \n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def train(self, datafolder ,epochs,batch_size,save_images_interval):\n",
    "        #Get the real images\n",
    "        training_data = self.get_training_data(datafolder)\n",
    "\n",
    "        #Map all values to a range between -1 and 1.\n",
    "        training_data = training_data / 127.5 - 1.\n",
    "\n",
    "        #Two arrays of labels. Labels for real images: [1,1,1 ... 1,1,1], labels for generated images: [0,0,0 ... 0,0,0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        mfid=[]\n",
    "        for epoch in range(epochs):\n",
    "            yr = np.ones((batch_size,1))\n",
    "            yr2=np.zeros((batch_size,classnum+1))\n",
    "            yr2[0:batch_size,0]=1\n",
    "            yf = np.zeros((batch_size,1))\n",
    "            yf2=np.zeros((batch_size,classnum+1))\n",
    "            yf2[0:batch_size,(epoch%classnum)+1]=1\n",
    "        \n",
    "        \n",
    "            ym = np.ones((batch_size,1))\n",
    "            ym2=np.zeros((batch_size,classnum+1))\n",
    "            ym2[0:batch_size,(epoch%classnum)+1]=1\n",
    "        \n",
    "            # Select a random half of images\n",
    "            \n",
    "            indices = np.random.randint(0,len(training_data),batch_size)\n",
    "            real_images = training_data[indices]\n",
    "\n",
    "            #Generate random noise for a whole batch.\n",
    "            random_noise = np.random.normal(0,1,(batch_size,self.random_noise_dimension))\n",
    "            #Generate a batch of new images.\n",
    "            generated_images = self.generator.predict(random_noise)\n",
    "\n",
    "            #Train the discriminator on real images.\n",
    "            discriminator_loss_real = self.discriminator.train_on_batch(real_images,[yr,yr2])\n",
    "            #Train the discriminator on generated images.\n",
    "            discriminator_loss_generated = self.discriminator.train_on_batch(generated_images,[yf,yf2])\n",
    "            #Calculate the average discriminator loss.\n",
    "            discriminator_loss = 0.5 * np.add(discriminator_loss_real,discriminator_loss_generated)\n",
    "\n",
    "            #Train the generator using the combined model. Generator tries to trick discriminator into mistaking generated images as real.\n",
    "            generator_loss = self.combined.train_on_batch(random_noise,[ym,ym2])\n",
    "            print (\"%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss: %f]\" % (epoch, discriminator_loss[0], 100*discriminator_loss[1], generator_loss[0]))\n",
    "            \n",
    "            images1 = scale_images(real_images, (16,16,3))\n",
    "            images2 = scale_images(generated_images, (16,16,3))\n",
    "\n",
    "            act1 = images1.reshape(batch_size,768  )\n",
    "            act2 = images2.reshape(batch_size,768  )\n",
    "            temp=get_fid(act1, act2)\n",
    "            print('fid : ' + str(temp))\n",
    "    \n",
    "            mfid.append(temp)\n",
    "\n",
    "            if epoch % save_images_interval == 0:\n",
    "                self.save_images(epoch)\n",
    "                \n",
    "\n",
    "        #Save the model for a later use\n",
    "        for i in range(0,len(mfid)):\n",
    "            print(mfid[i])\n",
    "        \n",
    "        print('----')\n",
    "        print(np.mean(mfid))\n",
    "        #self.generator.save(\"saved_models/facegenerator.h5\")\n",
    "\n",
    "    def save_images(self,epoch):\n",
    "        #Save 25 generated images for demonstration purposes using matplotlib.pyplot.\n",
    "        rows, columns = 5, 5\n",
    "        noise = np.random.normal(0, 1, (rows * columns, self.random_noise_dimension))\n",
    "        generated_images = self.generator.predict(noise)\n",
    "\n",
    "        generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "        figure, axis = plt.subplots(rows, columns)\n",
    "        image_count = 0\n",
    "        for row in range(rows):\n",
    "            for column in range(columns):\n",
    "                axis[row,column].imshow(generated_images[image_count, :], cmap='spring')\n",
    "                axis[row,column].axis('off')\n",
    "                image_count += 1\n",
    "        figure.savefig('./Imagenet/dogs2/generated_' + str(epoch)+ '.png')\n",
    "        #figure.show()\n",
    "        plt.close()\n",
    "\n",
    "    def generate_single_image(self,model_path,image_save_path):\n",
    "        noise = np.random.normal(0,1,(1,self.random_noise_dimension))\n",
    "        model = load_model(model_path)\n",
    "        generated_image = model.predict(noise)\n",
    "        #Normalized (-1 to 1) pixel values to the real (0 to 256) pixel values.\n",
    "        generated_image = (generated_image+1)*127.5\n",
    "        print(generated_image)\n",
    "        #Drop the batch dimension. From (1,w,h,c) to (w,h,c)\n",
    "        generated_image = np.reshape(generated_image,self.image_shape)\n",
    "\n",
    "        image = Image.fromarray(generated_image,\"RGB\")\n",
    "        #image.save(image_save_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    facegenerator = FaceGenerator(64,64,3)\n",
    "    facegenerator.train(datafolder=\"test\",epochs=5000, batch_size=32, save_images_interval=100)\n",
    "    #facegenerator.generate_single_image(\"saved_models/facegenerator.h5\",\"test.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
